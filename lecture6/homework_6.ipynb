{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Lfm2SHHrKO"
      },
      "source": [
        "## Домашнее задание: \"Детекция объектов на изображении\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBi6uhznYZc_"
      },
      "source": [
        "ФИО:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bAjQPD3H50I"
      },
      "source": [
        "## Цель задания:\n",
        "Научиться самостоятельно решать задачу детекции.\n",
        "## Описание задания:\n",
        "В рамках данного домашнего задания предлагается решить задачу детекции мячей и настроить полный пайплайн обучения модели.\n",
        "\n",
        "Процесс выполнения задания следующий:\n",
        "\n",
        "0. Выбор модели детекции для обучения:\n",
        "    1. Выберите модель детекции для выполнения домашнего задания. Любую, кроме RetinaNet. Ее реализацию можно взять из открытых источников. Модель можно брать предобученную ( в этом случае в пункте 4. показать влияние предобучения на финальное качество).\n",
        "    2. Полезные ссылки: [PyTorch Vision Models](https://pytorch.org/vision/stable/models.html) (блок Object Detection), [SOTA модели детекции](https://paperswithcode.com/sota/object-detection-on-coco), [Возможный пример кода](https://github.com/AlekseySpasenov/dl-course/blob/autumn_2023/lecture8/detection_example/pytorch_detection_workshop.ipynb)\n",
        "    3. Вы можете использовать RetinaNet, которая была реализована на семинаре, но это приведет к снижению оценки на **–2.5 балла**, так как задания 1.1 и 2.1 уже были выполнены в рамках занятия.\n",
        "\n",
        "1. Подготовка обучающего набора данных\n",
        "    0. Для выполнения задания используйте датасет с изображениями мячей, который использовался на семинаре.\n",
        "    1. Реализуйте корректный класс Dataset и Dataloader для выбранной модели (должен работать форвард вашей модели на том, что выходит из даталоадера) **0.5 балла**.\n",
        "    2. Добавьте простые аугментации в датасет (аугментации, не затрагивающие изменение ground-truth bounding box) **0.5 балла**.\n",
        "    3. Внедрите сложные аугментации (аугментации, затрагивающие изменение ground-truth bounding box. Например, аффинные преобразования: сдвиг, поворот и т.д.) **0.5 балла**.\n",
        "\n",
        "    4. Полезные ссылки: https://pytorch.org/vision/stable/transforms.html , https://albumentations.ai\n",
        "\n",
        "2. Реализация корректного train-loop и обучение модели:  \n",
        "    1. Реализуйте эффективный train-loop для вашей модели и проведите обучение **2 балла**.\n",
        "    2. Выполните несколько запусков обучения с различными параметрами, например: сравните влияние различных аугментаций, оцените влияние того была предобучена модель или нет, сравните результаты при изменении гиперпараметров итд (на ваш выбор) **0.5 балла**.\n",
        "\n",
        "3. Валидация обученных моделей на тестовой выборке, вычисление метрики mAP\n",
        "    1. Оцените качество моделей на тестовой части данных и рассчитайте метрику mAP **0.5 балл**\n",
        "    2. Полезные ссылки: [mean_average_precision](https://github.com/bes-dev/mean_average_precision)\n",
        "\n",
        "4. Выводы **0.5 балл**:\n",
        "    1. Проанализируйте результаты обучения, визуально оцените качество работы модели.\n",
        "    2. Прокомментируйте распространенные ошибки модели и предложите пути для улучшения финального решения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e0ec5svufBRd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = 'seminar_objdet_retina_oi5_ball'\n",
        "image_dir = os.path.join(base_dir, 'oi5_ball')\n",
        "train_json_path = os.path.join(base_dir, 'oi5_ball_filename_to_bbox_train.json')\n",
        "val_json_path = os.path.join(base_dir, 'oi5_ball_filename_to_bbox_val.json')\n",
        "\n",
        "with open(train_json_path, 'r') as f:\n",
        "    train_annotations = json.load(f)\n",
        "with open(val_json_path, 'r') as f:\n",
        "    val_annotations = json.load(f)\n",
        "\n",
        "annotations = {**train_annotations, **val_annotations}\n",
        "train_images = list(train_annotations.keys())\n",
        "val_images = list(val_annotations.keys())\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(val_images)\n",
        "val_size = int(0.7 * len(val_images))\n",
        "val_images, test_images = val_images[:val_size], val_images[val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BallDataset(Dataset):\n",
        "    def __init__(self, image_list, annotations, image_dir, transforms=None, is_train=True):\n",
        "        self.image_list = image_list\n",
        "        self.annotations = annotations\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_list[idx].split('/')[-1])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        width, height = img.size\n",
        "\n",
        "        bboxes = self.annotations.get(self.image_list[idx], [])\n",
        "        abs_bboxes = []\n",
        "        for bbox in bboxes:\n",
        "            xmin = bbox[0] * width\n",
        "            ymin = bbox[2] * height\n",
        "            xmax = bbox[1] * width\n",
        "            ymax = bbox[3] * height\n",
        "            x_min = min(xmin, xmax)\n",
        "            y_min = min(ymin, ymax)\n",
        "            x_max = max(xmin, xmax)\n",
        "            y_max = max(ymin, ymax)\n",
        "            if x_max > x_min and y_max > y_min:\n",
        "                abs_bboxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        num_objs = len(abs_bboxes)\n",
        "        labels = [1] * num_objs\n",
        "\n",
        "        if self.transforms and self.is_train and num_objs > 0:\n",
        "            img_np = np.array(img)\n",
        "            transformed = self.transforms(image=img_np, bboxes=abs_bboxes, class_labels=labels)\n",
        "            img = transformed['image'].float() / 255.0\n",
        "            abs_bboxes = transformed['bboxes']\n",
        "            labels = transformed['class_labels']\n",
        "        else:\n",
        "            img = torch.as_tensor(np.array(img), dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "\n",
        "        target = {\n",
        "            'boxes': torch.as_tensor(abs_bboxes, dtype=torch.float32) if abs_bboxes else torch.zeros((0, 4)),\n",
        "            'labels': torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros(0),\n",
        "            'image_id': torch.tensor([idx])\n",
        "        }\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\101\\source\\repos\\bmstu_basic_nn_2025\\.venv\\Lib\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "c:\\Users\\101\\source\\repos\\bmstu_basic_nn_2025\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ],
      "source": [
        "simple_transforms = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.HueSaturationValue(p=0.5),\n",
        "    A.GaussianBlur(p=0.3),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
        "\n",
        "complex_transforms = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset_simple = BallDataset(train_images, annotations, image_dir, transforms=simple_transforms, is_train=True)\n",
        "train_dataset_complex = BallDataset(train_images, annotations, image_dir, transforms=complex_transforms, is_train=True)\n",
        "val_dataset = BallDataset(val_images, annotations, image_dir, transforms=ToTensorV2(), is_train=False)\n",
        "test_dataset = BallDataset(test_images, annotations, image_dir, transforms=ToTensorV2(), is_train=False)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader_simple = DataLoader(train_dataset_simple, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
        "train_loader_complex = DataLoader(train_dataset_complex, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=1, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=1, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(pretrained=True):\n",
        "    model = ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1)\n",
        "    num_classes = 2  # ball + background\n",
        "    num_anchors = len(model.anchor_generator.aspect_ratios[0])  # 6 для SSDLite\n",
        "    \n",
        "    # Обновление classification_head\n",
        "    for i, seq in enumerate(model.head.classification_head.module_list):\n",
        "        if len(seq) >= 2 and isinstance(seq[1], torch.nn.Conv2d):\n",
        "            in_channels = seq[0][0].in_channels  # Извлекаем in_channels из первого слоя (Conv2dNormActivation)\n",
        "            # Заменяем второй слой на новый Conv2d для 2 классов\n",
        "            model.head.classification_head.module_list[i][1] = torch.nn.Conv2d(\n",
        "                in_channels, num_anchors * num_classes, kernel_size=3, padding=1\n",
        "            )\n",
        "    \n",
        "    model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\101\\AppData\\Local\\Temp\\ipykernel_8968\\308018599.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "scaler = GradScaler()\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_batches = len(data_loader)\n",
        "    \n",
        "    with tqdm(total=num_batches, desc=f'Epoch {epoch+1}/{10}', unit='batch') as pbar:\n",
        "        for i, (images, targets) in enumerate(data_loader):\n",
        "            valid_pairs = [(img, tgt) for img, tgt in zip(images, targets) if tgt['boxes'].shape[0] > 0]\n",
        "            if not valid_pairs:\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "            images = [pair[0].to(device) for pair in valid_pairs]\n",
        "            targets = [{k: v.to(device) for k, v in pair[1].items()} for pair in valid_pairs]\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss_dict = model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "            \n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "            batch_loss = losses.item()\n",
        "            running_loss += batch_loss\n",
        "            \n",
        "            vram_allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB\n",
        "            vram_used = vram_total - torch.cuda.memory_reserved() / 1024**2  # MB\n",
        "            \n",
        "            pbar.set_postfix({'Loss': f'{batch_loss:.4f}', \n",
        "                             'VRAM Used': f'{vram_used:.1f}MB/{vram_total:.1f}MB'})\n",
        "            pbar.update(1)\n",
        "    \n",
        "    avg_loss = running_loss / num_batches if num_batches > 0 else 0.0\n",
        "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def train_model(model, train_loader, num_epochs=10, lr=0.005):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0005)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "        losses.append(loss)\n",
        "        lr_scheduler.step()\n",
        "        torch.save(model.state_dict(), f'model_epoch_{epoch}.pth')\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches: 180\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of batches:\", len(train_loader_complex))\n",
        "for i, (images, targets) in enumerate(train_loader_complex):\n",
        "    print(f\"Batch {i+1}, Images shape: {images[0].shape if images else 'None'}, Targets: {targets[0]['boxes'].shape if targets[0]['boxes'].numel() > 0 else 'Empty'}\")\n",
        "    if i == 2: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:   0%|          | 0/180 [00:00<?, ?batch/s]"
          ]
        }
      ],
      "source": [
        "model1 = get_model(pretrained=True)\n",
        "losses1 = train_model(model1, train_loader_complex)\n",
        "plt.plot(losses1, label='Pretrained + Complex Aug')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2 = get_model(pretrained=False)\n",
        "losses2 = train_model(model2, train_loader_complex)\n",
        "plt.plot(losses2, label='Scratch + Complex Aug')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3 = get_model(pretrained=True)\n",
        "losses3 = train_model(model3, train_loader_simple)\n",
        "plt.plot(losses3, label='Pretrained + Simple Aug')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    metric = MeanAveragePrecision(iou_thresholds=[0.5])\n",
        "    model.eval()\n",
        "    for images, targets in data_loader:\n",
        "        valid_pairs = [(img, tgt) for img, tgt in zip(images, targets) if tgt['boxes'].shape[0] > 0]\n",
        "        if not valid_pairs:\n",
        "            continue\n",
        "        images = [img.to(device) for img, _ in valid_pairs]\n",
        "        tgts = [{k: v.to(device) for k, v in tgt.items()} for _, tgt in valid_pairs]\n",
        "        outputs = model(images)\n",
        "        metric.update(outputs, tgts)\n",
        "    return metric.compute()['map'].item()\n",
        "\n",
        "models = [model1, model2, model3]\n",
        "model_names = ['Pretrained + Complex', 'Scratch + Complex', 'Pretrained + Simple']\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    map_val = evaluate(model, val_loader, device)\n",
        "    map_test = evaluate(model, test_loader, device)\n",
        "    print(f\"{model_names[i]} - Val mAP: {map_val}, Test mAP: {map_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize(img, target, pred):\n",
        "    img = img.permute(1, 2, 0).cpu().numpy()\n",
        "    plt.imshow(img)\n",
        "    for box in target['boxes']:\n",
        "        plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, color='green'))\n",
        "    for i, box in enumerate(pred['boxes']):\n",
        "        if pred['scores'][i] > 0.5:\n",
        "            plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, color='red'))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "img, target = test_dataset[0]\n",
        "model1.eval()\n",
        "pred = model1([img.to(device)])[0]\n",
        "visualize(img, target, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzFYLD4wbByW"
      },
      "source": [
        "### Оставьте обратную связь по занятию №6\n",
        "Это поможет улучшить курс и сделать следующие занятия ещё лучше!\n",
        "Форма обратной связи: https://forms.gle/hTfRQaGBrqic7LyP6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCCFGDsbfH7y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU name: NVIDIA GeForce RTX 3070\n",
            "VRAM allocated: 0.0 MB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "print(\"VRAM allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
