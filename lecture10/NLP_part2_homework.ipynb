{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvgXC6YxSm12"
      },
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        return torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        return torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int\n",
        "    ) -> str:\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        beams = [(input_ids, 0.0)]\n",
        "        for _ in range(max_length):\n",
        "            new_beams = []\n",
        "            for cur_ids, log_prob in beams:\n",
        "                if cur_ids[0, -1].item() == self.tokenizer.eos_token_id:\n",
        "                    new_beams.append((cur_ids, log_prob))\n",
        "                    continue\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(cur_ids)\n",
        "                    logits = outputs.logits[:, -1, :]\n",
        "                probs = torch.log_softmax(logits, dim=-1)\n",
        "                top_probs, top_indices = torch.topk(probs, k=num_beams, dim=-1)\n",
        "                for i in range(num_beams):\n",
        "                    token_prob = top_probs[0, i].item()\n",
        "                    token_id = top_indices[0, i].item()\n",
        "                    new_token = torch.tensor([[token_id]], dtype=torch.long)\n",
        "                    new_ids = torch.cat([cur_ids, new_token], dim=-1)\n",
        "                    new_log_prob = log_prob + token_prob\n",
        "                    new_beams.append((new_ids, new_log_prob))\n",
        "            new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = new_beams[:num_beams]\n",
        "            if all(beam[0][0, -1].item() == self.tokenizer.eos_token for beam in beams):\n",
        "                break\n",
        "        best_sequence = beams[0][0].squeeze(0)\n",
        "        return self.tokenizer.decode(best_sequence, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 0.9) -> int:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
        "        filtered_logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "        return filtered_logits\n",
        "\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: int = 0) -> torch.Tensor:\n",
        "        if top_k == 0:\n",
        "            return logits\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        filtered_logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "        return filtered_logits\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "        if strategy == \"beam_search\":\n",
        "            return self._beam_search_generate(prompt, max_length, num_beams)\n",
        "\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        generated_ids = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(generated_ids)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            if temperature != 1.0:\n",
        "                next_token_logits = self.apply_temperature(next_token_logits, temperature)\n",
        "\n",
        "            if top_k > 0:\n",
        "                next_token_logits = self._apply_top_k(next_token_logits, top_k)\n",
        "\n",
        "            if top_p < 1.0:\n",
        "                next_token_logits = self._apply_top_p(next_token_logits, top_p)\n",
        "\n",
        "            if strategy == \"greedy\":\n",
        "                next_token_id = self.greedy_sampling(next_token_logits)\n",
        "\n",
        "            elif strategy == \"random\":\n",
        "                next_token_id = self.random_sampling(next_token_logits)\n",
        "\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "            next_token = torch.tensor([[next_token_id]], dtype=torch.long)\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=-1)\n",
        "\n",
        "            if next_token_id == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.tokenizer.decode(generated_ids.squeeze(0), skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed9feb08"
      },
      "source": [
        "model = Model()\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time in Hollywood, \""
      ],
      "metadata": {
        "id": "K_5_J_LQnJw1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ba218f"
      },
      "source": [
        "### 1. Greedy Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a9eb65a",
        "outputId": "78724421-e386-4a3e-dc19-40a89e14342b"
      },
      "source": [
        "greedy_output = model.generate(prompt, strategy=\"greedy\")\n",
        "print(greedy_output)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in Hollywood,  the first thing you would notice is that the actors were all wearing black.  The actors were all wearing black.  The actors were all wearing black.  The actors were all wearing black.  The actors were all wearing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "248d5249"
      },
      "source": [
        "### 2. Random Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fd3b394",
        "outputId": "e072ebe4-9dc4-432b-9e80-085104a4b7fb"
      },
      "source": [
        "random_output = model.generate(prompt, strategy=\"random\")\n",
        "print(random_output)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in Hollywood,  Mick Carter became estranged from Chip Cullen -- who now lives by the name \"Playboy\" -- , who read in a screen check \"Roger\" on that day with Carter in 2001. Mickey Cullen \\  was an accomplished playboy from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c3ea906"
      },
      "source": [
        "### 3. Temperature\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a76550",
        "outputId": "018b4b53-67c0-470f-f6f4-5fac9a25eecc"
      },
      "source": [
        "temp = model.generate(prompt, strategy=\"random\", temperature=0.5)\n",
        "print(temp)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in Hollywood,  the only way to be successful was to be like the rest of the world. People were curious, but they were not looking for another opportunity. So, in the fall of 2013, I started working on an application for an agency called Glam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ead300"
      },
      "source": [
        "### 4. Top-K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ec0dca",
        "outputId": "f85799d5-f4e1-4d97-e0b4-ceba3a158c07"
      },
      "source": [
        "topk = model.generate(prompt, strategy=\"random\", top_k=30)\n",
        "print(topk)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in Hollywood, _____________ was the most admired man in America with the world in his grip, the most respected journalist at the center of an organization that would make any man want to be a man again, and the most loved by the media and the nation, as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ce2a2f9"
      },
      "source": [
        "### 5. Top-P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b65638ad",
        "outputId": "dae02727-da0d-4ade-aea5-92f4d4b5a2fc"
      },
      "source": [
        "topp = model.generate(prompt, strategy=\"random\", top_p=0.75)\n",
        "print(topp)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in Hollywood,  Hollywood, based in the rural Southwest, decided to have a kid named Hank who lived in the hills.  But Hank went on to marry a woman he never knew existed.  And so it was that the sun shone on the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1988b636"
      },
      "source": [
        "### 6. Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21cb2ac1",
        "outputId": "84fba603-f211-4a36-f98f-8b84a3a5cefc"
      },
      "source": [
        "beam_search = model.generate(prompt, strategy=\"beam_search\", num_beams=10)\n",
        "print(beam_search)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in Hollywood,  there was a time when there was a time when there was a time when there was a time when there was a time when there was a time when there was a time when there was a time when there was a time when there was a time\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}