{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xRUXhCVUzur"
      },
      "source": [
        "# Домашнее задание \"NLP. Часть 1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koQiHQFT8XO7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUhaEvmpTCsv"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q88wy8uTDZh"
      },
      "outputs": [],
      "source": [
        "def normalize_pretokenize_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGDzAEpJT_zs"
      },
      "outputs": [],
      "source": [
        "# This block is for tests only\n",
        "test_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"never jump over the lazy dog quickly\",\n",
        "    \"brown foxes are quick and dogs are lazy\"\n",
        "]\n",
        "\n",
        "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        all_words.extend(words)\n",
        "    vocab = sorted(set(all_words))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, vocab_index\n",
        "\n",
        "vocab, vocab_index = build_vocab(test_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eemkFZ1tVLw4"
      },
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Реализовать One-Hot векторизацию текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qiw7w5OhTDeD"
      },
      "outputs": [],
      "source": [
        "def one_hot_vectorization(\n",
        "    text: str,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[List[int]]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    if vocab is None and vocab_index is None:\n",
        "        vocab = sorted(set(words))\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    elif vocab_index is None:\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return [list(map(int, np.eye(1, len(vocab_index), vocab_index[word]).flatten())) for word in words]\n",
        "    \n",
        "\n",
        "def test_one_hot_vectorization(\n",
        "    vocab: List[str],\n",
        "    vocab_index: Dict[str, int]\n",
        ") -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown fox\"\n",
        "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result[0]) != expected_length:\n",
        "            return False\n",
        "\n",
        "        words_in_text = normalize_pretokenize_text(text)\n",
        "        for i, word in enumerate(words_in_text):\n",
        "            if word in vocab_index:\n",
        "                idx = vocab_index[word]\n",
        "                if result[i][idx] != 1:\n",
        "                    return False\n",
        "\n",
        "        print(\"One-Hot-Vectors test PASSED\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2-LJcmbTe04"
      },
      "outputs": [],
      "source": [
        "assert test_one_hot_vectorization(vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAF8IOYMVT3s"
      },
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Реализовать Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_QjiviNBkbS"
      },
      "outputs": [],
      "source": [
        "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    return {word : words.count(word) for word in words}\n",
        "\n",
        "\n",
        "def test_bag_of_words_vectorization() -> bool:\n",
        "    try:\n",
        "        text = \"the the quick brown brown brown\"\n",
        "        result = bag_of_words_vectorization(text)\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            return False\n",
        "\n",
        "        if result.get('the', 0) != 2:\n",
        "            return False\n",
        "        if result.get('quick', 0) != 1:\n",
        "            return False\n",
        "        if result.get('brown', 0) != 3:\n",
        "            return False\n",
        "        if result.get('nonexistent', 0) != 0:\n",
        "            return False\n",
        "\n",
        "        print(\"Bad-of-Words test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScFuXh_9TtJm"
      },
      "outputs": [],
      "source": [
        "assert test_bag_of_words_vectorization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6LblWJfX2kr"
      },
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Реализовать TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqcMYJkrTlV0"
      },
      "outputs": [],
      "source": [
        "def tf_idf_vectorization(text: str, corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[float]:\n",
        "    # не знаю зачем нужен vocab_index в здесь ибо по нему просто не найти tf-idf\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    if vocab is None:\n",
        "        vocab = sorted(set(words))\n",
        "    tf = {word: words.count(word) / len(words) for word in set(words)}\n",
        "    total_docs = len(corpus)\n",
        "    idf = {\n",
        "        word: np.log(total_docs / (sum(1 for doc in corpus if word in normalize_pretokenize_text(doc)) + 1)) + 1 \n",
        "        for word in vocab\n",
        "    }\n",
        "    return [tf.get(word, 0) * idf.get(word, 0) for word in vocab]\n",
        "\n",
        "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown\"\n",
        "        result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"TF-IDF test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKIyS724T0XH"
      },
      "outputs": [],
      "source": [
        "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0f9FZCrX5_s"
      },
      "source": [
        "## Задание 4 (1 балл)\n",
        "Реализовать Positive Pointwise Mutual Information (PPMI).  \n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "$$PPMI(word, context) = max(0, PMI(word, context))$$\n",
        "$$PMI(word, context) = log \\frac{P(word, context)}{P(word) P(context)} = log \\frac{N(word, context)|(word, context)|}{N(word) N(context)}$$\n",
        "где $N(word, context)$ -- число вхождений слова $word$ в окно $context$ (размер окна -- гиперпараметр)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUg6K2-wTwr6"
      },
      "outputs": [],
      "source": [
        "def ppmi_vectorization(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2\n",
        ") -> List[float]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    if vocab_index is None:\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    co_occur = defaultdict(lambda: defaultdict(int))\n",
        "    word_freq = defaultdict(int)\n",
        "    for doc in corpus:\n",
        "        doc_words = normalize_pretokenize_text(doc)\n",
        "        for i, target in enumerate(doc_words):\n",
        "            word_freq[target] += 1\n",
        "            for j in range(max(0, i - window_size), min(len(doc_words), i + window_size + 1)):\n",
        "                if i != j:\n",
        "                    context = doc_words[j]\n",
        "                    co_occur[target][context] += 1\n",
        "    total_pairs = sum(sum(context.values()) for context in co_occur.values())\n",
        "    vector = [0.0] * len(vocab)\n",
        "    for word in vocab:\n",
        "        if word not in word_freq:\n",
        "            continue  \n",
        "        p_word = word_freq[word] / len(corpus)\n",
        "        ppmi_values = []\n",
        "        for context_word in words:\n",
        "            if context_word not in word_freq:\n",
        "                continue\n",
        "            p_context = word_freq[context_word] / len(corpus)\n",
        "            p_joint = co_occur[word][context_word] / total_pairs\n",
        "            if p_joint > 0:\n",
        "                pmi = math.log(p_joint / (p_word * p_context))\n",
        "                ppmi_values.append(max(0, pmi))\n",
        "        if ppmi_values:\n",
        "            vector[vocab_index[word]] = sum(ppmi_values) / len(ppmi_values)\n",
        "    return vector\n",
        "\n",
        "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"quick brown fox\"\n",
        "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"PPMI test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"PPMI test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgHmNZy75XFV"
      },
      "outputs": [],
      "source": [
        "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK29va3PBH_8"
      },
      "source": [
        "## Задание 5 (1 балл)\n",
        "Реализовать получение эмбеддингов из fasttext и bert (для bert лучше использовать CLS токен)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOe8dRLl5eqN"
      },
      "outputs": [],
      "source": [
        "def get_fasttext_embeddings(text: str, model_path: str = None, model: any = None) -> List[np.ndarray]:\n",
        "    if model is None:\n",
        "        if model_path is None:\n",
        "            raise ValueError(\"Ни импорта ни пути\")\n",
        "        model = fasttext.load_model(model_path)\n",
        "    words = text.split()\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in model.words:\n",
        "            embeddings.append(model[word])\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9GXy6n0AtsZ"
      },
      "outputs": [],
      "source": [
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    model_name: str = 'bert-base-uncased',\n",
        "    pool_method: str = 'cls'\n",
        ") -> np.ndarray:\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    return last_hidden_states[:, 0, :].numpy().squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_KoKolrD49R"
      },
      "source": [
        "## Задание 6 (1.5 балла)\n",
        "Реализовать обучение так, чтобы можно было поверх эмбеддингов, реализованных в предыдущих заданиях, обучить какую-то модель (вероятно неглубокую, например, CatBoost) на задаче классификации текстов ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsc98L8JE8G-"
      },
      "outputs": [],
      "source": [
        "def vectorize_dataset(\n",
        "    dataset_name: str = \"imdb\",\n",
        "    vectorizer_type: str = \"bow\",\n",
        "    split: str = \"train\",\n",
        "    sample_size: int = 2500\n",
        ") -> Tuple[Any, List, List]:\n",
        "\n",
        "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
        "\n",
        "    if sample_size:\n",
        "        dataset = dataset.select(range(min(sample_size, len(dataset))))\n",
        "\n",
        "    texts = [item['text'] for item in dataset if 'text' in item and item['text'].strip()]\n",
        "    labels = [item['label'] for item in dataset if 'label' in item]\n",
        "\n",
        "    def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "        all_words = []\n",
        "        for text in texts:\n",
        "            words = normalize_pretokenize_text(text)\n",
        "            all_words.extend(words)\n",
        "        vocab = sorted(set(all_words))\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "        return vocab, vocab_index\n",
        "\n",
        "    vocab, vocab_index = build_vocab(texts)\n",
        "\n",
        "    vectorized_data = []\n",
        "    for text in texts:\n",
        "        if vectorizer_type == \"one_hot\":\n",
        "            vectorized_data.append(one_hot_vectorization(text, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"bow\":\n",
        "            bow_dict = bag_of_words_vectorization(text)\n",
        "            vector = [bow_dict.get(word, 0) for word in vocab]\n",
        "            vectorized_data.append(vector)\n",
        "        elif vectorizer_type == \"tfidf\":\n",
        "            vectorized_data.append(tf_idf_vectorization(text, texts, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"ppmi\":\n",
        "            vectorized_data.append(ppmi_vectorization(text, texts, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"fasttext\":\n",
        "            embeddings = get_fasttext_embeddings(text)\n",
        "            if embeddings:\n",
        "                avg_embedding = np.mean(embeddings, axis=0)\n",
        "                vectorized_data.append(avg_embedding.tolist())\n",
        "            else:\n",
        "                vectorized_data.append([0] * 300)\n",
        "        elif vectorizer_type == \"bert\":\n",
        "            embedding = get_bert_embeddings(text)\n",
        "            vectorized_data.append(embedding.tolist())\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")\n",
        "    print(vectorized_data, labels)\n",
        "    return vectorized_data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRRw01XiBg6H"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "def train(\n",
        "    embeddings_method=\"bow\",\n",
        "    test_size=0.2,\n",
        "    val_size=0.2,\n",
        "    cv_folds=5\n",
        "):\n",
        "    X_train, y_train = vectorize_dataset(\"imdb\", embeddings_method, \"train\", sample_size=2500)\n",
        "    X_test, y_test = vectorize_dataset(\"imdb\", embeddings_method, \"test\", sample_size=500)\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, \n",
        "        test_size=val_size, \n",
        "        random_state=42,\n",
        "        stratify=y_train\n",
        "    )\n",
        "    model = CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=5, random_seed=42, verbose=False)\n",
        "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=True)\n",
        "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "    print(f\"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"\\nMethod: {embeddings_method}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test F1-score: {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "naMqAkjqFHAe"
      },
      "outputs": [],
      "source": [
        "for embeddings_method in [\"bow\", \"one_hot\", \"tfidf\", \"ppmi\", \"fasttext\", \"bert\"]:\n",
        "    train(embeddings_method=embeddings_method)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "12venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
