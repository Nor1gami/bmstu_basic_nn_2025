{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvgXC6YxSm12"
      },
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        # your code here\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        # your code here\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int\n",
        "    ) -> str:\n",
        "        # your code here\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        # your code here\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "        # your code here\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_k: float = self.vocab_size) -> torch.Tensor:\n",
        "        # your code here\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "        # your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNUHC3UmSYd-"
      },
      "outputs": [],
      "source": [
        "# Продемонстрируйте результат работы `generate` при различных параметрах"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
