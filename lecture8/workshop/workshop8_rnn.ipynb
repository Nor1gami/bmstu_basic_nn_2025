{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Семинар 8: RNN в задаче Language Modeling\n",
        "\n",
        "**Language Modeling** — это задача предсказания следующего слова или символа в тексте на основе предыдущего контекста.\n",
        "\n",
        "Примеры задач:\n",
        "- Предсказать следующее слово: `\"I like to drink ___\"` → `\"coffee\"`\n",
        "- Предсказать следующий символ: `\"RNNs a\"` → `\"r\"`\n",
        "\n",
        "\n",
        "Зачем нужна задача Language Modeling?\n",
        "\n",
        "1. **Автодополнение и генерация текста.**  \n",
        "   Модель учится предсказывать, что будет дальше — это основа T9, autocomplete, генераторов текста и кода.\n",
        "\n",
        "2. **Обучение языковых моделей.**  \n",
        "   GPT, BERT и другие начинают обучение именно с предсказания следующего токена.\n",
        "\n",
        "3. **Понимание структуры языка.**  \n",
        "   Через предсказание следующего символа или слова модель изучает грамматику, синтаксис и типичные шаблоны языка.\n",
        "\n",
        "Мы будем решать задачу предсказания следующего символа\n",
        "\n",
        "\n",
        "![img](https://github.com/stepanovnick/dl-course/blob/main/lecture8/workshop/images/char.png?raw=true)\n"
      ],
      "metadata": {
        "id": "oTN1yC7B_St6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "# textwrap\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "J86Ynro5RVfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "Deu1lrkdRhpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.1 Генерируем текст для обучения**"
      ],
      "metadata": {
        "id": "G9aIuskmQ91N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy-example\n",
        "text = \"\"\"\n",
        "Recurrent Neural Networks (RNNs) are a type of neural network architecture which is mainly used\n",
        "to detect patterns in a sequence of data. Such data can be handwriting, genomes, text or numerical\n",
        "time series which are often produced in industry settings (e.g. stock markets or sensors).\n",
        "However, they are also applicable to images if these get respectively decomposed into a series of\n",
        "patches and treated as a sequence\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "77VmrOK3_7Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Сохраняем алфавит (вместо словаря)\n",
        "chars = sorted(list(set(text)))\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "itos"
      ],
      "metadata": {
        "id": "ziAtxky7_9Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(s):\n",
        "  return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
        "def decode(ix):\n",
        "  return \"\".join(itos[i] for i in ix)\n"
      ],
      "metadata": {
        "id": "81sHIfFgABQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = encode(text)"
      ],
      "metadata": {
        "id": "rNKZHBYpAJqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "tNWqhUxuAMIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharSeqDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ### Делим текст на отрезки\n",
        "        x = self.data[idx: idx+self.seq_len]\n",
        "        y = self.data[idx + 1: idx+self.seq_len + 1] ### таргет - следующий символ\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "2qIhYkOO_Vmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 32\n",
        "batch_size = 16\n",
        "\n",
        "ds = CharSeqDataset(data, seq_len)\n",
        "dl = DataLoader(ds, batch_size=batch_size, shuffle=True, )\n"
      ],
      "metadata": {
        "id": "REUzr4VL_y7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(dl))\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "8M-L1SHHCyl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "id": "2BK_lTqrEneV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2 Embedings или векторизация слов**\n",
        "\n",
        "После токенизации каждое слово или символ заменяется на целочисленный индекс. Но нейросеть не может \"понять\" числа как слова — ей нужны **векторы признаков**.\n",
        "\n",
        "Слой `nn.Embedding(num_tokens, embed_dim)` создаёт **матрицу эмбеддингов**:\n",
        "\n",
        "- Каждому индексу соответствует **вектор признаков** (например, размером 100).\n",
        "- Эти векторы **обучаются вместе с моделью**.\n",
        "- Похожие слова получают **похожие векторы**.\n",
        "\n",
        "По сути, это **обучаемый словарь**: индекс → вектор\n",
        "\n",
        "\n",
        "![img](https://github.com/stepanovnick/dl-course/blob/main/lecture8/workshop/images/emb.png?raw=true)\n",
        "\n",
        "Будем использовать это дальше в модели!\n",
        "\n"
      ],
      "metadata": {
        "id": "tgLApHnhEnvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3 Vanilla RNN**\n",
        "\n",
        "Форвард на шаге t:\n",
        "$$\n",
        "a_t = W_{xh} x_t + W_{hh} h_{t-1} + b_h,\\quad\n",
        "h_t = \\tanh(a_t),\\quad\n",
        "o_t = W_{hy} h_t + b_y\n",
        "$$\n",
        "\n",
        "Где $x_t$ — эмбеддинг символа (используем `nn.Embedding`), $h_t$ — скрытое состояние.\n",
        "\n",
        "Обучаем по кросс-энтропии на каждом t. Полный градиент — это обычный backprop **через развёртку по времени** (BPTT).\n",
        "Чтобы стабилизировать:\n",
        "- **orthogonal** инициализация для \\(W_{hh}\\),\n",
        "- **gradient clipping** (например, 1.0)."
      ],
      "metadata": {
        "id": "5KZpCNLCAfuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNNCell(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        ### Инициализируем обучаемые вектора - эмбединги.\n",
        "        ### Каждый вектор имеет размер embed_size. Всего vocab_size векторов.\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        self.Wxh = nn.Linear(embed_size, hidden_size, bias=True)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.Why = nn.Linear(hidden_size, output_size, bias=True)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # self.hidden_size — размер скрытого состояния\n",
        "        return torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.Wxh.weight)\n",
        "        nn.init.orthogonal_(self.Whh.weight)\n",
        "        nn.init.xavier_uniform_(self.Why.weight)\n",
        "\n",
        "        nn.init.zeros_(self.Wxh.bias)\n",
        "        nn.init.zeros_(self.Whh.bias)\n",
        "        nn.init.zeros_(self.Why.bias)\n",
        "\n",
        "    def forward_step(self, x_t, h_prev):\n",
        "\n",
        "        x_e = self.embed(x_t)\n",
        "\n",
        "        a_t = self.Wxh(x_e) + self.Whh(h_prev)\n",
        "        h_t = torch.tanh(a_t) # скрытое состояние\n",
        "\n",
        "        o_t = self.Why(h_t) # логиты\n",
        "        return o_t, h_t\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        # x: [B, T]\n",
        "        B, T = x.size(0), x.size(1)\n",
        "        H = self.hidden_size\n",
        "\n",
        "        ### Инициализируем начальное состояние как 0-вектор\n",
        "        h = x.new_zeros(B, H, dtype=torch.float32, device=x.device) if h0 is None else h0\n",
        "\n",
        "        logits = []\n",
        "        for t in range(T):\n",
        "            o_t, h = self.forward_step(x[:, t], h) # передаем состояние h с предыдущего шага\n",
        "            logits.append(o_t)\n",
        "\n",
        "        return torch.stack(logits, dim=1), h  # [B,T,V], h_T\n"
      ],
      "metadata": {
        "id": "X37n2BeoAa2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.4 Обучение**\n",
        "\n",
        "Perplexity  — это метрика, которая используется для количественной оценки того, насколько хорошо вероятностная модель предсказывает выборку. Perplexity (перевод: недоумение, растерянность) является мерой **среднего числа вариантов, из которых модель должна была бы выбирать на каждом шаге**.\n",
        "\n",
        "$$ PPL = \\exp(Loss)$$\n",
        "\n",
        "![img](https://github.com/stepanovnick/dl-course/blob/main/lecture8/workshop/images/perplexity.png?raw=true)\n"
      ],
      "metadata": {
        "id": "A5wrhTjeQ6hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, max_norm=1.0):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)  # [B,T]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, *_ = model(x)              # [B,T,V]\n",
        "        loss = loss_fn(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        ### Боримся с Gradient Exploding:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.numel()   # суммируем по всем позициям\n",
        "        total_tokens += x.numel()\n",
        "        # print(total_tokens, x.numel())\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            # print(x.shape, y.shape)\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits, _ = model(x)\n",
        "\n",
        "            loss = loss_fn(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "            total_loss += loss.item() * x.numel()\n",
        "            total_tokens += x.numel()\n",
        "            # print(total_tokens, x.numel())\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n"
      ],
      "metadata": {
        "id": "VuUdY_ZUB4DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xluV4IywCM0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 32\n",
        "hidden_size = 128\n",
        "\n",
        "rnn = VanillaRNNCell(vocab_size, embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "F8-yv5ZNCM43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 8\n",
        "lr = 3e-3\n",
        "\n",
        "opt_rnn  = optim.Adam(rnn.parameters(),  lr=lr)"
      ],
      "metadata": {
        "id": "OB-lNmKbCM7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"== Train RNN ==\")\n",
        "for e in range(1, epochs+1):\n",
        "    tr_loss, tr_ppl = train_epoch(rnn, dl, opt_rnn, max_norm=1.0)\n",
        "    ev_loss, ev_ppl = evaluate(rnn, dl)\n",
        "    print(f\"[RNN] epoch {e:02d} | train loss {tr_loss:.4f} ppl {tr_ppl:.2f} | eval loss {ev_loss:.4f} ppl {ev_ppl:.2f}\")\n"
      ],
      "metadata": {
        "id": "P2ZZA2t1CRS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0_bE9K0CRVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.5 Предсказание модели**\n",
        "\n",
        "На этапе инференса мы подаём модели начальную строку (например, `\"RNN model is\"`). Модель возвращает распределение вероятностей для следующего символа. На его основе мы выбираем символ, добавляем его в строку — и подаём обратно. Так, шаг за шагом, строится текст.\n",
        "\n",
        "Простейший вариант — **argmax**.\n",
        "\n",
        "![img](https://github.com/stepanovnick/dl-course/blob/main/lecture8/workshop/images/words.png?raw=true)"
      ],
      "metadata": {
        "id": "KsFnE4oIMzDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_argmax(model, start_text, length=100):\n",
        "    model.eval()\n",
        "    x = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "\n",
        "    out = list(start_text)\n",
        "    ch = x[0, -1].unsqueeze(0)  # берем последний символ\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            logits, hidden = model.forward_step(ch.unsqueeze(0), hidden) ### подаем символ ch\n",
        "            ch = torch.argmax(logits, dim=-1).squeeze(0) # извлекам предсказанный символ -> сохраняем в ch\n",
        "            out.append(itos[ch.item()])\n",
        "\n",
        "    return \"\".join(out)\n"
      ],
      "metadata": {
        "id": "3N9hw-_qMynB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_argmax(rnn, start_text=\"RNN model is\", length=200))"
      ],
      "metadata": {
        "id": "DoBGE3JvMyqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_argmax(rnn, start_text=\"a dog\", length=200))"
      ],
      "metadata": {
        "id": "RII0oaiNNjkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проблемы:\n",
        "- текст полностью предопределен начальным сиволом,\n",
        "- может зацикливаться."
      ],
      "metadata": {
        "id": "mzK-xz1ZOqfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.6 Семплирование**\n",
        "\n",
        "Вместо argmax можно выбрать следующий символ **случайно** — из распределения вероятностей, которое даёт softmax.  \n",
        "Это помогает избежать зацикливания и делает текст разнообразнее.\n",
        "\n",
        "Перед softmax можно поделить логиты на **temperature** — она управляет «остротой» распределения:\n",
        "\n",
        "$$\\text{softmax}(o_i / T) = \\frac{\\exp(o_i / T)}{\\sum_j \\exp(o_j / T)} $$\n",
        "\n",
        "- \\(T < 1\\) — модель становится более уверенной,  \n",
        "- \\(T > 1\\) — выбор становится более случайным.\n",
        "\n",
        "![img](https://github.com/stepanovnick/dl-course/blob/main/lecture8/workshop/images/temp.png?raw=true)\n",
        "\n",
        "Далее можно взять случайный сивол из распределения. НО полное семплирование по всему словарю может давать странные выборы (в том числе мусор). Чтобы ограничить это:\n",
        "\n",
        "- **top-k sampling**: выбираем \\( k \\) самых вероятных токенов, обнуляем остальные, нормируем и семплируем из них.\n",
        "\n",
        "- **top-p (nucleus) sampling**: выбираем минимальный набор токенов, чья суммарная вероятность ≥ \\( p \\) (например, 0.9).  \n",
        "  Размер пула динамический, зависит от уверенности модели.\n"
      ],
      "metadata": {
        "id": "NOcYKjmJKEnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(model, start_text, length=100, temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    x = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "\n",
        "    out = list(start_text)\n",
        "    ch = x[0, -1].unsqueeze(0) # берем последний символ\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            logits, hidden = model.forward_step(ch.unsqueeze(0), hidden)\n",
        "\n",
        "            ### Преобразование\n",
        "            logits = logits.squeeze(0).squeeze(0) / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            ### Выбираем топ k и нормируем\n",
        "            if top_k:\n",
        "                v, i = torch.topk(probs, top_k)\n",
        "                probs = torch.zeros_like(probs).scatter(0, i, v)\n",
        "                probs /= probs.sum()\n",
        "\n",
        "            ### Выбираем 1 индекс с вероятностью из probs\n",
        "            ch = torch.multinomial(probs, 1)\n",
        "            out.append(itos[ch.item()])\n",
        "\n",
        "    return \"\".join(out)\n"
      ],
      "metadata": {
        "id": "scNR3Le_CRX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample(rnn, start_text=\"RNN model is\", length=200, temperature=0.8, top_k=10)\n"
      ],
      "metadata": {
        "id": "Wb72VaRIRvoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8TNv912vZ1vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.7 Модель с LSTM**"
      ],
      "metadata": {
        "id": "0LvpZ1T_LGF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) # num_layers - определяет кол-во слоев LSTM стакнутых вместе\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embed(x)  # [B, T, E]\n",
        "        out, hidden = self.lstm(x, hidden)  # out: [B, T, H]\n",
        "        logits = self.fc(out)  # [B, T, V]\n",
        "        return logits, hidden\n",
        "\n",
        "    def forward_step(self, x_t, hidden):\n",
        "        x_e = self.embed(x_t)  # [B, E]\n",
        "        x_e = x_e.unsqueeze(1)  # [B, 1, E] — нужно 3D на вход LSTM\n",
        "        out, hidden = self.lstm(x_e, hidden)  # out: [B, 1, H]\n",
        "        logits = self.fc(out.squeeze(1))  # [B, V]\n",
        "        return logits, hidden\n",
        "\n"
      ],
      "metadata": {
        "id": "wi_jpG1SbKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ?nn.LSTM"
      ],
      "metadata": {
        "id": "w5C6rCnFCBY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 32\n",
        "hidden_size = 128\n",
        "\n",
        "rnn_lstm = LSTMModel(vocab_size, embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "aPjbSWbhCBbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 8\n",
        "lr = 3e-3\n",
        "\n",
        "opt_rnn  = optim.Adam(rnn_lstm.parameters(),  lr=lr)"
      ],
      "metadata": {
        "id": "-Hu_khtiCBdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"== Train LSTM ==\")\n",
        "for e in range(1, epochs+1):\n",
        "    tr_loss, tr_ppl = train_epoch(rnn_lstm, dl, opt_rnn, max_norm=1.0)\n",
        "    ev_loss, ev_ppl = evaluate(rnn_lstm, dl)\n",
        "    print(f\"[LSTM] epoch {e:02d} | train loss {tr_loss:.4f} ppl {tr_ppl:.2f} | eval loss {ev_loss:.4f} ppl {ev_ppl:.2f}\")\n"
      ],
      "metadata": {
        "id": "7Xzy0sIBU0Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G94OUBDDU0OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-e5eABwU0Qu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}